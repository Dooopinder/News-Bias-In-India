# -*- coding: utf-8 -*-
"""midsempresentation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Tjqd3X4BPtaEmd8grz3kNykkMOfMTW45

ACCESSING NEWS API
"""

import requests
import pandas as pd

# API key
api_key = 'api key'  # Replace with your Mediastack API key

# function to fetch articles with a dynamic offset
def fetch_articles(offset):
    url = "http://api.mediastack.com/v1/news"
    params = {
        'access_key': api_key,                       # Your API Key
        'countries': 'in',                            # Country: India
        'categories': 'general,business,science,technology',  # Include categories that might have political bias
        'keywords': 'Politics,government,election,policy',  # Query for political-related news
        'languages': 'en',                           # Language filter: English
        'limit': 100,                                # Max number of articles per request (100 articles)
        'offset': offset,                            # Skip the first 'offset' articles
    }

    # Make the request to Mediastack API
    response = requests.get(url, params=params)

    # Check if the request was successful
    if response.status_code == 200:
        data = response.json()
        articles = data['data']

        # Create an empty list to store article details
        article_data = []

        # Loop through the articles and extract required details
        for article in articles:
            article_data.append({
                'Title': article['title'],
                'Description': article.get('description', 'No description'),
                'Content': article.get('content', 'No content'),
                'Published At': article['published_at'],
                'Source': article['source'],
                'URL': article['url']
            })

        # Convert the list into a DataFrame
        df = pd.DataFrame(article_data)

        # Save to a CSV file with the offset number in the filename
        filename = f"political_articles_offset_{offset}.csv"
        df.to_csv(filename, index=False)
        print(f"Saved to {filename}")

        # Return the DataFrame for further processing (if needed)
        return df
    else:
        print(f"Error fetching data: {response.status_code}")
        return pd.DataFrame()  # Return an empty DataFrame on error

# Example usage: Fetch and save multiple sets of articles using different offsets
def fetch_and_save_all_articles():
    all_data = pd.DataFrame()  # Initialize an empty DataFrame to hold all data

    # Loop over multiple offsets (0, 100, 200, etc.)
    for offset in range(0,100):  # Change range as per your need (this example pulls 1 batch)
        df = fetch_articles(offset)
        all_data = pd.concat([all_data, df], ignore_index=True)  # Append the new data

    # Optionally save all the data combined into one CSV file
    all_data.to_csv('combined_political_articles.csv', index=False)
    print("All articles combined and saved as combined_political_articles.csv")

# Run the function to fetch and save all articles
fetch_and_save_all_articles()

"""Preprocessing"""

import os
import requests
import pandas as pd
from newspaper import Article
import re

# Load the dataset with correct encoding
df = pd.read_csv('/content/updated_political_articles.csv', encoding='ISO-8859-1')


# Function to extract content from article URL
def extract_content(url):
    try:
        article = Article(url)
        article.download()
        article.parse()
        return article.text
    except Exception as e:
        print(f"Error extracting content from {url}: {e}")
        return None

# Function to clean text
def clean_text(text):
    if not isinstance(text, str):
        return ""
    text = re.sub(r'[^\x00-\x7F]+', '', text)
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'http\S+', '', text)
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)
    return text

# Heuristic-based classifier
def classify_bias(text, source):
    text = text.lower()

    if source in ownership_bias_map:
        return ownership_bias_map[source]
    if any(keyword in text for keyword in left_keywords):
        return 'Left-wing'
    elif any(keyword in text for keyword in right_keywords):
        return 'Right-wing'
    elif any(keyword in text for keyword in neutral_keywords):
        return 'Neutral'
    return 'Neutral'

# Extract content if missing
if 'Content' not in df.columns or df['Content'].isnull().all():
    df['Content'] = df['URL'].apply(extract_content)
else:
    df['Content'] = df['Content'].fillna('').apply(clean_text)

# Only classify if bias column is missing or empty
def update_bias(row):
    current_bias = str(row['Political Bias']).strip()
    if current_bias == '' or current_bias.lower() == 'none' or pd.isna(current_bias):
        return classify_bias(row['Content'], row['Source'])
    else:
        return current_bias

df['Political Bias'] = df.apply(update_bias, axis=1)

# Save output
output_path = '/content/sample_data/final_manual_bias.csv'
df.to_csv(output_path, index=False, encoding='utf-8')
print(f" File saved: {output_path}")

"""MODEL MAKING"""

import pandas as pd
import joblib
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.utils import resample
from textblob import TextBlob
import scipy

# Step 1: Load the dataset
df = pd.read_csv('/content/sample_data/final_manual_bias.csv')

# Step 2: Clean and normalize 'Political Bias' column
df['Political Bias'] = df['Political Bias'].str.strip().str.capitalize()

# Step 3: Drop rows with missing 'Content' or 'Political Bias'
df.dropna(subset=['Content', 'Political Bias'], inplace=True)

# Step 4: Add sentiment polarity score
df['Sentiment'] = df['Content'].apply(lambda text: TextBlob(str(text)).sentiment.polarity)

# Step 5: Heuristic flipping (criticism detection)
# Since Right-wing is in power, strong criticism often comes from the Left
def flip_bias_based_on_negative_terms(text, bias):
    text = str(text).lower()

    # Criticism of the government or right-wing ideology
    anti_right_terms = ['fascist', 'bhakt', 'saffron terror', 'hindutva', 'right-wing hate', 'dictatorship', 'modi failure', 'bjp corruption', 'godi media']

    # Criticism of the left or opposition
    anti_left_terms = ['leftist propaganda', 'corrupt left', 'communist agenda', 'anti-national liberals', 'tukde tukde gang']

    if bias == 'Right-wing' and any(term in text for term in anti_right_terms):
        return 'Left-wing'
    elif bias == 'Left-wing' and any(term in text for term in anti_left_terms):
        return 'Right-wing'
    return bias

df['Political Bias'] = df.apply(lambda row: flip_bias_based_on_negative_terms(row['Content'], row['Political Bias']), axis=1)

# Step 6: Balance the dataset
df_majority = df[df['Political Bias'] == 'Right-wing']
df_left = df[df['Political Bias'] == 'Left-wing']
df_neutral = df[df['Political Bias'] == 'Neutral']

df_left_upsampled = resample(df_left, replace=True, n_samples=len(df_majority), random_state=42)
df_neutral_upsampled = resample(df_neutral, replace=True, n_samples=len(df_majority), random_state=42)

df_balanced = pd.concat([df_majority, df_left_upsampled, df_neutral_upsampled])

# Step 7: Feature and label split
X_text = df_balanced['Content']
X_sentiment = df_balanced['Sentiment']
y = df_balanced['Political Bias']

# Step 8: TF-IDF Vectorization
vectorizer = TfidfVectorizer(stop_words='english', max_features=5000, ngram_range=(1, 2))
X_text_tfidf = vectorizer.fit_transform(X_text)

# Step 9: Combine text features with sentiment
X_combined = scipy.sparse.hstack([X_text_tfidf, X_sentiment.values.reshape(-1, 1)])

# Step 10: Train/Test split
X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)

# Step 11: Train the model
model = LogisticRegression(max_iter=1000, class_weight='balanced')
model.fit(X_train, y_train)

# Step 12: Evaluate
y_pred = model.predict(X_test)
print("\n Classification Report:\n")
print(classification_report(y_test, y_pred))

# Step 13: Save model and vectorizer
joblib.dump(model, '/content/sample_data/political_bias_model.pkl')
joblib.dump(vectorizer, '/content/sample_data/tfidf_vectorizer.pkl')
print("Model and vectorizer saved to /content/sample_data")

"""MODEL TEST"""

import joblib
import pandas as pd
import numpy as np
from textblob import TextBlob
import scipy

# Load the saved model and vectorizer
model = joblib.load('/content/sample_data/political_bias_model.pkl')
vectorizer = joblib.load('/content/sample_data/tfidf_vectorizer.pkl')

def classify_political_bias(user_input, input_type='content'):
    # Step 1: Vectorize the text input
    X_tfidf = vectorizer.transform([user_input])

    # Step 2: Calculate sentiment polarity score
    sentiment = TextBlob(user_input).sentiment.polarity
    X_sentiment = np.array([[sentiment]])

    # Step 3: Combine text vector and sentiment into one feature set
    X_combined = scipy.sparse.hstack([X_tfidf, X_sentiment])

    # Step 4: Predict
    prediction = model.predict(X_combined)[0]
    prediction_prob = model.predict_proba(X_combined)[0]

    # Step 5: Map class probabilities
    class_probs = dict(zip(model.classes_, prediction_prob))

    # Display result
    print(f"\n Prediction Probabilities:")
    for label, prob in class_probs.items():
        print(f"{label}: {prob:.2%}")

    print(f"\n Predicted Political Bias based on {input_type.capitalize()}: {prediction}")
    return prediction, class_probs

# Example usage
sample_article = "The government is determined to suppress the protests using any means necessary."
classify_political_bias(sample_article, 'content')

"""END USER"""

import joblib
import numpy as np
from textblob import TextBlob
import scipy

# Load the saved model and vectorizer
model = joblib.load('/content/sample_data/political_bias_model.pkl')
vectorizer = joblib.load('/content/sample_data/tfidf_vectorizer.pkl')

def classify_political_bias(user_input, input_type):
    # Vectorize text input
    X_tfidf = vectorizer.transform([user_input])

    # Calculate sentiment score
    sentiment_score = TextBlob(user_input).sentiment.polarity
    X_sentiment = np.array([[sentiment_score]])

    # Combine TF-IDF with sentiment score
    X_combined = scipy.sparse.hstack([X_tfidf, X_sentiment])

    # Predict
    prediction = model.predict(X_combined)[0]
    prediction_prob = model.predict_proba(X_combined)[0]

    print(f"\nPrediction Probabilities:")
    for label, prob in zip(model.classes_, prediction_prob):
        print(f"{label}: {prob:.2f}")

    print(f"\n Predicted Political Bias based on {input_type.capitalize()}: {prediction}")

# Loop to classify multiple entries
while True:
    print("\nChoose the input type:")
    print("1. Title")
    print("2. Content")
    print("3. Exit")

    choice = input("Enter '1' for Title, '2' for Content, or '3' to Exit: ")

    if choice == '1':
        user_input = input("Enter article title: ")
        classify_political_bias(user_input, 'title')
    elif choice == '2':
        user_input = input("Enter article content: ")
        classify_political_bias(user_input, 'content')
    elif choice == '3':
        print("Exiting. Thank you!")
        break
    else:
        print("Invalid choice. Please try again.")

"""EXAMPLE TITLES

Opposition Slams Government Over Unemployment Crisis

PM Modi: Nation First, Politics Later

Sepak Takraw World Cup 2025: India wins historic Gold
"""